{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVEle7cvLU__"
      },
      "source": [
        "# Dubbing Assistant\n",
        "\n",
        "1. Extract the audio from the video.\n",
        "2. Separate the audio into vocals and accompaniment.\n",
        "3. Transcribe the vocal. (Optional)\n",
        "4. Translate the transcription. (Optional)\n",
        "5. Text to speech\n",
        "5. Clone the voice. (Future)\n",
        "6. Use the cloned voice to read the translated script. (Future)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXSjnkb_Esmc"
      },
      "source": [
        "# Subtitle (YouTube)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhZBp9uutGyo"
      },
      "source": [
        "If subtitles are available, we can skip the transcription and translation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-1_ixTjhLBD"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install anvil-uplink"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMGvYkSzhVsG",
        "outputId": "2f968a0a-1565-4d8c-930e-d7638e43df39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connecting to wss://anvil.works/uplink\n",
            "Anvil websocket open\n",
            "Connected to \"Default Environment\" as SERVER\n"
          ]
        }
      ],
      "source": [
        "import anvil.server\n",
        "\n",
        "anvil.server.connect(\"server_2LPJFY3R2XW5CBQFJ2YEAVJI-MBGHABK6RX4NM5UT\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ljVyXHDExZO"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install youtube-transcript-api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EC1jMldCTFcB"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install pytube"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tzAV5UUTPi-"
      },
      "outputs": [],
      "source": [
        "from pytube import YouTube"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYdCxea9TQz0"
      },
      "outputs": [],
      "source": [
        "@anvil.server.callable\n",
        "def get_video_title_from_url(url):\n",
        "  try:\n",
        "    yt = YouTube(url)\n",
        "    return yt.title\n",
        "  except:\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jf5XXkkGIgg"
      },
      "outputs": [],
      "source": [
        "from youtube_transcript_api import YouTubeTranscriptApi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpgFUaS6Ez2W"
      },
      "outputs": [],
      "source": [
        "# Get YouTube video id from its URL\n",
        "def get_video_id_from_url(url):\n",
        "  if \"youtube.com/watch?v=\" in url:\n",
        "    video_id = url.split(\"v=\")[1]\n",
        "    if (\"&\" in video_id):\n",
        "      video_id = video_id.split(\"&\")[0]\n",
        "    return video_id\n",
        "  elif (\"https://youtu.be/\" in url):\n",
        "    video_id = url.split(\"https://youtu.be/\")[1]\n",
        "    video_id = video_id.split(\"?\")[0]\n",
        "    return video_id\n",
        "  elif (\"youtube.com/shorts/\" in url):\n",
        "    video_id = url.split('youtube.com/shorts/')[1]\n",
        "    if ('?' in video_id):\n",
        "      video_id = video_id.split('?')[0]\n",
        "    return video_id\n",
        "  else:\n",
        "    # Handle invalid URL\n",
        "    print(\"Invalid URL!\")\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6heomM8wGW_g"
      },
      "outputs": [],
      "source": [
        "def YouTube_transcript(video_id):\n",
        "  try:\n",
        "    transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)\n",
        "  except:\n",
        "    transcript_list = []\n",
        "\n",
        "  return transcript_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-F3T-gBGbpA9"
      },
      "outputs": [],
      "source": [
        "# print(YouTube_transcript('2bdvoVr64HI'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7oGKWwnvmPh"
      },
      "outputs": [],
      "source": [
        "def video_Language_Code(numberOfTranscript, transcript_list):\n",
        "  videoLanguageCode = \"\" # language-code of the default language of the video\n",
        "  # translatableList = []\n",
        "  if (numberOfTranscript > 0):\n",
        "    for transcript in transcript_list: # iterate over all available transcripts\n",
        "      if transcript.is_translatable and (\"(auto-generated)\" in transcript.language):\n",
        "          videoLanguageCode = transcript.language_code\n",
        "          break\n",
        "    if videoLanguageCode == \"\":\n",
        "      for transcript in transcript_list:\n",
        "        if transcript.is_translatable:\n",
        "          videoLanguageCode = transcript.language_code\n",
        "          # translatableList.append((transcript.language, transcript.language_code))\n",
        "          break\n",
        "  return videoLanguageCode"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def video_Language_Code2(numberOfTranscript, transcript_list):\n",
        "  videoLanguageCode = \"\" # language-code of the default language of the video\n",
        "  translatableList = []\n",
        "  if (numberOfTranscript > 0):\n",
        "    for transcript in transcript_list: # iterate over all available transcripts\n",
        "      if transcript.is_translatable and (\"(auto-generated)\" in transcript.language):\n",
        "          videoLanguageCode = transcript.language_code\n",
        "          break\n",
        "    if videoLanguageCode == \"\":\n",
        "      for transcript in transcript_list:\n",
        "        if transcript.is_translatable:\n",
        "          translatableList.append((transcript.language, transcript.language_code))\n",
        "          # break\n",
        "  if videoLanguageCode == '':\n",
        "    if len(translatableList) == 1:\n",
        "      videoLanguageCode = translatableList[0][1]\n",
        "      return videoLanguageCode\n",
        "    else:\n",
        "      return translatableList\n",
        "  else:\n",
        "    return videoLanguageCode"
      ],
      "metadata": {
        "id": "KVYRDq8CDcbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0b3ClwZRcEh"
      },
      "outputs": [],
      "source": [
        "def fetchTranscript(code, transcript_list):\n",
        "  selectedTranscript = \"\"\n",
        "  selectedTranscript = transcript_list.find_transcript([code]).fetch()\n",
        "\n",
        "  return selectedTranscript"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSEBntC_q4IW"
      },
      "outputs": [],
      "source": [
        "def fetchTranslation(videoLanguageCode, targetLanguageCode, transcript_list):\n",
        "  selectedTranslation = \"\"\n",
        "  selectedTranscript = transcript_list.find_transcript([videoLanguageCode])\n",
        "  selectedTranslation = selectedTranscript.translate(targetLanguageCode).fetch()\n",
        "\n",
        "  return selectedTranslation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def get_transcript2(url, code):\n",
        "#   video_id = get_video_id_from_url(url)\n",
        "#   transcript_list = YouTube_transcript(video_id)\n",
        "#   # print(transcript_list)\n",
        "#   selectedTranscript = []\n",
        "#   if code == 'original':\n",
        "#     try:\n",
        "#       numberOfTranscript = len(list(enumerate(transcript_list)))\n",
        "#       videoLanguageCode = video_Language_Code(numberOfTranscript, transcript_list)\n",
        "#     except:\n",
        "#       return None\n",
        "#     return videoLanguageCode"
      ],
      "metadata": {
        "id": "46QctsdNyVEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krxz-V2mpoCv"
      },
      "outputs": [],
      "source": [
        "@anvil.server.callable\n",
        "def get_transcript(url, code):\n",
        "  video_id = get_video_id_from_url(url)\n",
        "  transcript_list = YouTube_transcript(video_id)\n",
        "  # print(transcript_list)\n",
        "  selectedTranscript = []\n",
        "  if code == 'original':\n",
        "    try:\n",
        "      numberOfTranscript = len(list(enumerate(transcript_list)))\n",
        "      videoLanguageCode = video_Language_Code2(numberOfTranscript, transcript_list)\n",
        "      if numberOfTranscript == 0:\n",
        "        return []\n",
        "      if isinstance(videoLanguageCode, list):\n",
        "        return {'multiple': 'yes', 'options': videoLanguageCode}\n",
        "      else:\n",
        "        selectedTranscript = fetchTranscript(videoLanguageCode, transcript_list)\n",
        "    except:\n",
        "      return []\n",
        "\n",
        "  elif code == 'zh-Hans':\n",
        "    try:\n",
        "      selectedTranscript = fetchTranscript(code, transcript_list)\n",
        "    except:\n",
        "      try:\n",
        "        selectedTranscript = fetchTranscript('zh-Hant', transcript_list)\n",
        "      except:\n",
        "        try:\n",
        "          selectedTranscript = fetchTranscript('zh-CN', transcript_list)\n",
        "        except:\n",
        "          try:\n",
        "            selectedTranscript = fetchTranscript('zh-TW', transcript_list)\n",
        "          except:\n",
        "            try:\n",
        "              numberOfTranscript = len(list(enumerate(transcript_list)))\n",
        "              videoLanguageCode = video_Language_Code(numberOfTranscript, transcript_list)\n",
        "              selectedTranscript = fetchTranslation(videoLanguageCode, code, transcript_list)\n",
        "            except:\n",
        "              return []\n",
        "  else:\n",
        "    try:\n",
        "      selectedTranscript = fetchTranscript(code, transcript_list)\n",
        "    except:\n",
        "      try:\n",
        "        numberOfTranscript = len(list(enumerate(transcript_list)))\n",
        "        videoLanguageCode = video_Language_Code(numberOfTranscript, transcript_list)\n",
        "        selectedTranscript = fetchTranslation(videoLanguageCode, code, transcript_list)\n",
        "      except:\n",
        "        return []\n",
        "  for line in selectedTranscript:\n",
        "    line['speaker'] = ''\n",
        "    line['end'] = line['start'] + line['duration']\n",
        "    line['start'] = round(line['start'])\n",
        "    line['end'] = round(line['end'])\n",
        "    line['startMin'] = line['start']//60\n",
        "    line['startSec'] = line['start']%60\n",
        "    line['endMin'] = line['end']//60\n",
        "    line['endSec'] = line['end']%60\n",
        "  keys_to_remove = ['start', 'end', 'duration']\n",
        "  for line in selectedTranscript:\n",
        "    for key in keys_to_remove:\n",
        "      if key in line:\n",
        "        del line[key]\n",
        "  return selectedTranscript"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0rFcwLJzHBQ"
      },
      "outputs": [],
      "source": [
        "# print(get_transcript('https://youtu.be/2bdvoVr64HI?si=d6IUsv4vJCnw_Qxm', 'original'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(get_transcript('https://www.youtube.com/watch?v=XG7NfTwuGGs', 'de'))"
      ],
      "metadata": {
        "id": "HgW1zTvaC15j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAb5KdFS6s8T"
      },
      "source": [
        "# Download and split the YT audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bhxMgu_aHum"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "# !pip install pytube"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWuR3phTQXy5"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install pydub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSKPZNNGaDJ_"
      },
      "outputs": [],
      "source": [
        "from pytube import YouTube\n",
        "from pydub import AudioSegment\n",
        "import subprocess\n",
        "import os\n",
        "import requests\n",
        "import base64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1s3bIW4Sf-Ws"
      },
      "outputs": [],
      "source": [
        "def audioSplitPoints(duration_ms):\n",
        "  split_points = [0]  # Split points in milliseconds\n",
        "\n",
        "  if (duration_ms > 120*1000):\n",
        "    duration_left = duration_ms\n",
        "    point = 0\n",
        "    while (duration_left > 120*1000):\n",
        "      point = point + 120000\n",
        "      duration_left = duration_left - 120000\n",
        "      split_points.append(point)\n",
        "\n",
        "    if (duration_left <= 60000):\n",
        "      split_points.pop()\n",
        "\n",
        "  return split_points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-t_MMOLdZHYP"
      },
      "outputs": [],
      "source": [
        "def splitAudio(split_points, audio, duration_ms):\n",
        "  audio_segments = []\n",
        "  wav_files = []\n",
        "\n",
        "  if (len(split_points) > 1):\n",
        "    for i in range(len(split_points) - 1):\n",
        "      start_time = split_points[i]\n",
        "      end_time = split_points[i + 1]\n",
        "      segment = audio[start_time:end_time]\n",
        "      audio_segments.append(segment)\n",
        "\n",
        "    segment = audio[split_points[-1]:duration_ms]\n",
        "    audio_segments.append(segment)\n",
        "\n",
        "    os.chdir(\"/content/audio\")\n",
        "    for i, segment in enumerate(audio_segments):\n",
        "      output_file = f'segment_{i + 1}.wav'\n",
        "      segment.export(output_file, format='wav')\n",
        "      # print(f\"Segment {i + 1} saved as {output_file}\")\n",
        "      wav_files.append(output_file)\n",
        "\n",
        "    os.system(\"rm audio.wav\")\n",
        "\n",
        "  elif (len(split_points) == 1):\n",
        "    os.chdir(\"/content/audio\")\n",
        "    os.rename('audio.wav', 'segment_1.wav')\n",
        "    wav_files.append(\"segment_1.wav\")\n",
        "\n",
        "  return wav_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4ts8zjiogKT"
      },
      "outputs": [],
      "source": [
        "@anvil.server.callable\n",
        "def YTdownloadSplitAudio(url):\n",
        "  print(\"Starting...\")\n",
        "  yt = YouTube(url)\n",
        "  audio_resolution_list = []\n",
        "  for stream in yt.streams.filter(adaptive=True, mime_type=\"audio/mp4\"):\n",
        "    audio_resolution_list.append(int(stream.abr[:-4]))\n",
        "  audio_resolution = str(max(audio_resolution_list)) + \"kbps\"\n",
        "  Itag_audio = \"\"\n",
        "  total_size_audio = \"\"\n",
        "  for stream in yt.streams.filter(adaptive=True, mime_type=\"audio/mp4\", abr=audio_resolution):\n",
        "    Itag_audio = stream.itag\n",
        "  stream_audio = yt.streams.get_by_itag(Itag_audio)\n",
        "  directory_name = \"audio\"\n",
        "  if not os.path.exists(directory_name):\n",
        "    os.system(f\"mkdir {directory_name}\")\n",
        "  print(\"Downloading audio...\")\n",
        "  stream_audio.download(output_path=\"/content/audio\", filename=\"audio.wav\")\n",
        "  print(\"Downloaded\")\n",
        "  audio_file = '/content/audio/audio.wav'\n",
        "  audio = AudioSegment.from_file(audio_file)\n",
        "  duration_ms = len(audio)\n",
        "  split_points = audioSplitPoints(duration_ms)\n",
        "  print(\"Splitting...\")\n",
        "  wav_files = splitAudio(split_points, audio, duration_ms)\n",
        "  return wav_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdXoUjk3yXFs"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install demucs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dr6g4omdsKd9"
      },
      "outputs": [],
      "source": [
        "# Separate the audio into vocals and accompaniment\n",
        "@anvil.server.callable\n",
        "def vocalSeparation(url):\n",
        "  wav_files = YTdownloadSplitAudio(url)\n",
        "  print(\"Splitting stage completed\")\n",
        "  demucs_command = \"demucs --two-stems vocals -n htdemucs_ft\"\n",
        "  demucs_counter = 0\n",
        "  input_vocals = []\n",
        "  input_non_vocals = []\n",
        "\n",
        "  os.chdir(\"/content/audio\")\n",
        "  print(\"Separating vocal and non-vocal...\")\n",
        "  for wav_file in wav_files:\n",
        "      # Construct the full command\n",
        "      full_command = f\"{demucs_command} {wav_file}\"\n",
        "\n",
        "      # Execute the command\n",
        "      try:\n",
        "          subprocess.run(full_command, shell=True, check=True)\n",
        "          # print(f\"Processing {wav_file} completed successfully.\")\n",
        "          input_vocals.append(f'/content/audio/separated/htdemucs_ft/{wav_file[:-4]}/vocals.wav')\n",
        "          input_non_vocals.append(f'/content/audio/separated/htdemucs_ft/{wav_file[:-4]}/no_vocals.wav')\n",
        "          demucs_counter = demucs_counter + 1\n",
        "      except subprocess.CalledProcessError as e:\n",
        "          print(f\"Error processing {wav_file}: {e}\")\n",
        "\n",
        "  if (demucs_counter == len(wav_files)):\n",
        "    os.chdir(\"/content\")\n",
        "    return [input_vocals, input_non_vocals]\n",
        "\n",
        "  else:\n",
        "    os.chdir(\"/content\")\n",
        "    return \"Separation is unsuccessful\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezUwj3aqrDQQ"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJl9Eevu5GB1"
      },
      "outputs": [],
      "source": [
        "# Combining the splitted segments back into a long audio file\n",
        "@anvil.server.callable\n",
        "def combineAudio(url):\n",
        "  input = vocalSeparation(url)\n",
        "  print(\"Separation stage completed\")\n",
        "  print(\"Recombining splitted segments...\")\n",
        "  input_vocals = input[0]\n",
        "  input_non_vocals = input[1]\n",
        "\n",
        "  directory_name = \"combinedAudio\"\n",
        "  if not os.path.exists(directory_name):\n",
        "    os.system(f\"mkdir {directory_name}\")\n",
        "\n",
        "  audio_track = []\n",
        "  for i in input_vocals:\n",
        "    audio_track.append(AudioSegment.from_file(i))\n",
        "\n",
        "  vocal = audio_track[0]\n",
        "  for j in range(len(audio_track)):\n",
        "    if (j > 0):\n",
        "      vocal = vocal + audio_track[j]\n",
        "  vocal.export(\"/content/combinedAudio/vocal.wav\", format=\"wav\")\n",
        "  vocal = AudioSegment.from_wav(\"/content/combinedAudio/vocal.wav\")\n",
        "  vocal.export(\"/content/combinedAudio/vocal.mp3\", format=\"mp3\")\n",
        "\n",
        "  audio_track = []\n",
        "  for i in input_non_vocals:\n",
        "    audio_track.append(AudioSegment.from_file(i))\n",
        "\n",
        "  nonVocal = audio_track[0]\n",
        "  for j in range(len(audio_track)):\n",
        "    if (j > 0):\n",
        "      nonVocal = nonVocal + audio_track[j]\n",
        "  nonVocal.export(\"/content/combinedAudio/nonVocal.wav\", format=\"wav\")\n",
        "  nonVocal = AudioSegment.from_wav(\"/content/combinedAudio/nonVocal.wav\")\n",
        "  nonVocal.export(\"/content/combinedAudio/nonVocal.mp3\", format=\"mp3\")\n",
        "  os.system(\"rm -r /content/audio\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msYMVUGQddFk"
      },
      "outputs": [],
      "source": [
        "# !rm -r /content/audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZHP-y1LNEhM"
      },
      "outputs": [],
      "source": [
        "# combineAudio(\"https://www.youtube.com/watch?v=s6jDsPXmo2I\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxbNBlaNuuxV"
      },
      "source": [
        "# Upload accompaniment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mq3YgSjuwV2_"
      },
      "outputs": [],
      "source": [
        "from anvil.tables import app_tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNnrEFJGvhWx"
      },
      "outputs": [],
      "source": [
        "@anvil.server.callable\n",
        "def upload_accompaniment_vocal(videoUrl, videoTitle):\n",
        "  videoID = get_video_id_from_url(videoUrl)\n",
        "  videoUrl = 'https://www.youtube.com/watch?v=' + videoID\n",
        "  videoRow = app_tables.videos.get(youTubeVideoID=videoID)\n",
        "  if videoRow is None:\n",
        "    app_tables.videos.add_row(videoTitle=videoTitle, youTubeVideoID=videoID, videoUrl=videoUrl)\n",
        "  videoRow = app_tables.videos.get(youTubeVideoID=videoID)\n",
        "  if videoRow['accompaniment'] is None:\n",
        "    with open(\"/content/combinedAudio/nonVocal.mp3\", \"rb\") as f:\n",
        "      audioBytes = f.read()\n",
        "    audio = anvil.BlobMedia(content_type=\"audio/mp3\", content=audioBytes, name='accompaniment.mp3')\n",
        "    videoRow.update(accompaniment=audio)\n",
        "  if videoRow['vocal'] is None:\n",
        "    with open(\"/content/combinedAudio/vocal.mp3\", \"rb\") as f:\n",
        "      audioBytes = f.read()\n",
        "    audio = anvil.BlobMedia(content_type=\"audio/mp3\", content=audioBytes, name='vocal.mp3')\n",
        "    videoRow.update(vocal=audio)\n",
        "  os.system(\"rm -r /content/combinedAudio\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def upload_accompaniment_vocal2(videoUrl, videoTitle):\n",
        "  videoID = get_video_id_from_url(videoUrl)\n",
        "  videoUrl = 'https://www.youtube.com/watch?v=' + videoID\n",
        "  videoRow = app_tables.videos.get(youTubeVideoID=videoID)\n",
        "  if videoRow is None:\n",
        "    app_tables.videos.add_row(videoTitle=videoTitle, youTubeVideoID=videoID, videoUrl=videoUrl)\n",
        "  videoRow = app_tables.videos.get(youTubeVideoID=videoID)\n",
        "  if videoRow['accompaniment'] is None:\n",
        "    with open(\"/content/combinedAudio/nonVocal.mp3\", \"rb\") as f:\n",
        "      audioBytes = f.read()\n",
        "    audio = anvil.BlobMedia(content_type=\"audio/mp3\", content=audioBytes, name='accompaniment.mp3')\n",
        "    videoRow.update(accompaniment=audio)\n",
        "  if videoRow['vocal'] is None:\n",
        "    with open(\"/content/combinedAudio/vocal.mp3\", \"rb\") as f:\n",
        "      audioBytes = f.read()\n",
        "    audio = anvil.BlobMedia(content_type=\"audio/mp3\", content=audioBytes, name='vocal.mp3')\n",
        "    videoRow.update(vocal=audio)\n",
        "  # os.system(\"rm -r /content/combinedAudio\")"
      ],
      "metadata": {
        "id": "f5EZ5UYhJ8R6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iv5N58z2M-sh"
      },
      "source": [
        "# Transcribe the vocal using Whisper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2TVmcuteLc2"
      },
      "outputs": [],
      "source": [
        "# !mkdir /content/combineAudio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUd5_A2Hlem_"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!sudo apt update && sudo apt install ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RcBa11dY8Cw7"
      },
      "outputs": [],
      "source": [
        "import whisper\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import json"
      ],
      "metadata": {
        "id": "1jF5pzs7iSVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CS8nJf65F8g-"
      },
      "outputs": [],
      "source": [
        "import anvil.media"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WY550ziocedp"
      },
      "outputs": [],
      "source": [
        "# !whisper --model large-v2 --output_dir /content --output_format json --task translate /content/combineAudio/vocal.mp3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCAmj8-tNMaw"
      },
      "outputs": [],
      "source": [
        "# !whisper --help"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2BG4uZxmJII"
      },
      "outputs": [],
      "source": [
        "@anvil.server.callable\n",
        "def transcribe(url, videoTitle):\n",
        "  if os.path.exists('/content/combinedAudio'):\n",
        "    os.system('rm -r /content/combinedAudio')\n",
        "\n",
        "  videoID = get_video_id_from_url(url)\n",
        "  videoRow = app_tables.videos.get(youTubeVideoID=videoID)\n",
        "  if videoRow is not None:\n",
        "    if videoRow['vocal'] is not None:\n",
        "      os.system(f\"mkdir /content/combinedAudio\")\n",
        "      vocal = AudioSegment.from_file(io.BytesIO(videoRow['vocal'].get_bytes()), format=\"mp3\")\n",
        "      vocal.export(\"/content/combinedAudio/vocal.wav\", format=\"wav\")\n",
        "    else:\n",
        "      combineAudio(url)\n",
        "  else:\n",
        "    combineAudio(url)\n",
        "\n",
        "  upload_accompaniment_vocal2(url, videoTitle)\n",
        "  print(\"Transcribing...\")\n",
        "  if os.path.exists(\"/content/vocal.json\"):\n",
        "    os.remove(\"/content/vocal.json\")\n",
        "\n",
        "  transcript_command = 'whisper --model large-v2 --output_dir /content --output_format json --task transcribe /content/combinedAudio/vocal.wav'\n",
        "  subprocess.run(transcript_command, shell=True, check=True)\n",
        "\n",
        "  with open('vocal.json', 'r') as file:\n",
        "    # Load the JSON data into a Python dictionary\n",
        "    transcript = json.load(file)\n",
        "\n",
        "  transcriptGenerated = []\n",
        "  for segment in transcript['segments']:\n",
        "    transcriptGenerated.append({\"start\": segment['start'], \"end\": segment['end'], \"text\": segment['text']})\n",
        "\n",
        "  print(\"\\nTranscription stage completed\")\n",
        "  for line in transcriptGenerated:\n",
        "    line['speaker'] = ''\n",
        "    line['start'] = round(line['start'])\n",
        "    line['end'] = round(line['end'])\n",
        "    line['startMin'] = line['start']//60\n",
        "    line['startSec'] = line['start']%60\n",
        "    line['endMin'] = line['end']//60\n",
        "    line['endSec'] = line['end']%60\n",
        "  keys_to_remove = ['start', 'end']\n",
        "  for line in transcriptGenerated:\n",
        "    for key in keys_to_remove:\n",
        "      if key in line:\n",
        "        del line[key]\n",
        "  return transcriptGenerated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-_QK6_Dhmor"
      },
      "outputs": [],
      "source": [
        "# transcribe('https://www.youtube.com/shorts/ikjrjIwGcNY', '金門的年輕人在大選前如何掙扎？－ BBC News 中文')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMg4ErbYnIXR"
      },
      "source": [
        "# Conversion between transcript list and Anvil media and SRT/TXT file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bZ1ik_pLw4K"
      },
      "outputs": [],
      "source": [
        "@anvil.server.callable\n",
        "def convert_transcript_to_anvil_media(transcript):\n",
        "  srt_content = \"\"\n",
        "  counter = 1\n",
        "\n",
        "  for line in transcript:\n",
        "    line['startMin'] = int(line['startMin'])\n",
        "    line['startSec'] = int(line['startSec'])\n",
        "    line['endMin'] = int(line['endMin'])\n",
        "    line['endSec'] = int(line['endSec'])\n",
        "    start_time = \"{:02d}:{:02d}:{:02d},{}\".format(0, line['startMin'], line['startSec'], 0)\n",
        "    end_time = \"{:02d}:{:02d}:{:02d},{}\".format(0, line['endMin'], line['endSec'], 0)\n",
        "\n",
        "    srt_content += str(counter) + \"\\n\"\n",
        "    srt_content += start_time + \" --> \" + end_time + \"\\n\"\n",
        "    srt_content += line['text'] + \"\\n\\n\"\n",
        "    counter += 1\n",
        "\n",
        "  with open('transcript.srt', 'w') as file:\n",
        "    file.write(srt_content)\n",
        "\n",
        "  with open('transcript.srt', 'rb') as f:\n",
        "    transcriptByte = f.read()\n",
        "  transcriptMedia = anvil.BlobMedia(content_type=\"text/plain\", content=transcriptByte, name=\"transcript.srt\")\n",
        "  return transcriptMedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbgBdpmz4baS"
      },
      "outputs": [],
      "source": [
        "@anvil.server.callable\n",
        "def convert_transcript_to_txt(transcript):\n",
        "  srt_content = \"\"\n",
        "  counter = 1\n",
        "\n",
        "  for line in transcript:\n",
        "    start_time = \"{:02d}:{:02d}:{:02d},{}\".format(0, line['startMin'], line['startSec'], 0)\n",
        "    end_time = \"{:02d}:{:02d}:{:02d},{}\".format(0, line['endMin'], line['endSec'], 0)\n",
        "\n",
        "    srt_content += str(counter) + \"\\n\"\n",
        "    srt_content += start_time + \" --> \" + end_time + \"\\n\"\n",
        "    srt_content += line['text'] + \"\\n\\n\"\n",
        "    counter += 1\n",
        "\n",
        "  with open('transcript.txt', 'w') as file:\n",
        "    file.write(srt_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOtudtMbNk0W"
      },
      "outputs": [],
      "source": [
        "@anvil.server.callable\n",
        "def convert_srt_to_list(srt_file):\n",
        "  transcript_list = []\n",
        "  with open(srt_file, 'r') as file:\n",
        "    lines = file.readlines()\n",
        "    i = 0\n",
        "    while i < len(lines):\n",
        "      line = lines[i].strip()\n",
        "      if line.isdigit():\n",
        "        i += 1  # Skip the number line\n",
        "        time_line = lines[i].strip().split(\" --> \")\n",
        "        start_time = time_line[0].split(':')\n",
        "        end_time = time_line[1].split(':')\n",
        "        start_min = int(start_time[1])\n",
        "        end_min = int(end_time[1])\n",
        "        start_sec, start_mic_sec = map(int, start_time[2].split(',')[0:2])\n",
        "        end_sec, end_mic_sec = map(int, end_time[2].split(',')[0:2])\n",
        "        start_sec = round(start_sec + start_mic_sec/1000)\n",
        "        end_sec = round(end_sec + end_mic_sec/1000)\n",
        "        text = lines[i + 1].strip()\n",
        "        transcript_list.append({\n",
        "            'speaker': '',\n",
        "            'startMin': start_min,\n",
        "            'startSec': start_sec,\n",
        "            'endMin': end_min,\n",
        "            'endSec': end_sec,\n",
        "            'text': text\n",
        "        })\n",
        "        i += 2\n",
        "      else:\n",
        "        i += 1\n",
        "  return transcript_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exMDWRXgd86N"
      },
      "outputs": [],
      "source": [
        "# Example usage:\n",
        "# srt_file = \"transcript.srt\"\n",
        "# transcript_list = convert_srt_to_list(srt_file)\n",
        "# for line in transcript_list:\n",
        "#   print(line)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6j9glbU3NQz"
      },
      "source": [
        "\n",
        "# Translate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ohX9r1Q9n9-"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install deep-translator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3thGRMdG-_S1"
      },
      "outputs": [],
      "source": [
        "from deep_translator import GoogleTranslator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQ-IUozt4vn5"
      },
      "outputs": [],
      "source": [
        "@anvil.server.callable\n",
        "def translate_txt(transcript, targetLanguageCode):\n",
        "  convert_transcript_to_txt(transcript)\n",
        "\n",
        "  if targetLanguageCode == 'zh-Hans':\n",
        "    targetLanguageCode = 'zh-CN'\n",
        "\n",
        "  translate_srt = GoogleTranslator(source='auto', target=targetLanguageCode).translate_file('transcript.txt')\n",
        "  with open('translation.srt', 'w') as file:\n",
        "    file.write(translate_srt)\n",
        "\n",
        "  if os.path.exists(\"/content/vocal.json\"):\n",
        "    os.remove(\"/content/vocal.json\")\n",
        "  if os.path.exists(\"/content/transcript.txt\"):\n",
        "    os.remove(\"/content/transcript.txt\")\n",
        "\n",
        "  translation = convert_srt_to_list('translation.srt')\n",
        "  if os.path.exists(\"/content/transcript.srt\"):\n",
        "    os.remove(\"/content/transcript.srt\")\n",
        "\n",
        "  os.remove('/content/translation.srt')\n",
        "\n",
        "  return translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dATQaBL8nAeu"
      },
      "outputs": [],
      "source": [
        "# print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4vbzS0CLQS2"
      },
      "source": [
        "#Text to Speech"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNNFmQrvLUs6"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install edge-tts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdkONAfnv_pI"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import io"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDMWoYABMQ5Q"
      },
      "outputs": [],
      "source": [
        "def execute_command(command):\n",
        "  try:\n",
        "    output = subprocess.check_output(command, shell=True, universal_newlines=True)\n",
        "    # Split the output into a list of strings\n",
        "    output_list = output.splitlines()\n",
        "    return output_list\n",
        "  except subprocess.CalledProcessError as e:\n",
        "    print(\"Error executing command:\", e)\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7CgI5R6wJ-3",
        "outputId": "5ffa736e-9490-4ef7-8e85-8123d425abff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'language': 'af', 'country': 'za', 'name': 'af-ZA-AdriNeural', 'gender': 'f'}, {'language': 'af', 'country': 'za', 'name': 'af-ZA-WillemNeural', 'gender': 'm'}, {'language': 'am', 'country': 'et', 'name': 'am-ET-AmehaNeural', 'gender': 'm'}, {'language': 'am', 'country': 'et', 'name': 'am-ET-MekdesNeural', 'gender': 'f'}, {'language': 'ar', 'country': 'ae', 'name': 'ar-AE-FatimaNeural', 'gender': 'f'}, {'language': 'ar', 'country': 'ae', 'name': 'ar-AE-HamdanNeural', 'gender': 'm'}, {'language': 'ar', 'country': 'bh', 'name': 'ar-BH-AliNeural', 'gender': 'm'}, {'language': 'ar', 'country': 'bh', 'name': 'ar-BH-LailaNeural', 'gender': 'f'}, {'language': 'ar', 'country': 'dz', 'name': 'ar-DZ-AminaNeural', 'gender': 'f'}, {'language': 'ar', 'country': 'dz', 'name': 'ar-DZ-IsmaelNeural', 'gender': 'm'}, {'language': 'ar', 'country': 'eg', 'name': 'ar-EG-SalmaNeural', 'gender': 'f'}, {'language': 'ar', 'country': 'eg', 'name': 'ar-EG-ShakirNeural', 'gender': 'm'}, {'language': 'ar', 'country': 'iq', 'name': 'ar-IQ-BasselNeural', 'gender': 'm'}, {'language': 'ar', 'country': 'iq', 'name': 'ar-IQ-RanaNeural', 'gender': 'f'}, {'language': 'ar', 'country': 'jo', 'name': 'ar-JO-SanaNeural', 'gender': 'f'}, {'language': 'ar', 'country': 'jo', 'name': 'ar-JO-TaimNeural', 'gender': 'm'}, {'language': 'ar', 'country': 'kw', 'name': 'ar-KW-FahedNeural', 'gender': 'm'}, {'language': 'ar', 'country': 'kw', 'name': 'ar-KW-NouraNeural', 'gender': 'f'}, {'language': 'ar', 'country': 'lb', 'name': 'ar-LB-LaylaNeural', 'gender': 'f'}, {'language': 'ar', 'country': 'lb', 'name': 'ar-LB-RamiNeural', 'gender': 'm'}, {'language': 'ar', 'country': 'ly', 'name': 'ar-LY-ImanNeural', 'gender': 'f'}, {'language': 'ar', 'country': 'ly', 'name': 'ar-LY-OmarNeural', 'gender': 'm'}, {'language': 'ar', 'country': 'ma', 'name': 'ar-MA-JamalNeural', 'gender': 'm'}, {'language': 'ar', 'country': 'ma', 'name': 'ar-MA-MounaNeural', 'gender': 'f'}, {'language': 'ar', 'country': 'om', 'name': 'ar-OM-AbdullahNeural', 'gender': 'm'}, {'language': 'ar', 'country': 'om', 'name': 'ar-OM-AyshaNeural', 'gender': 'f'}, {'language': 'ar', 'country': 'qa', 'name': 'ar-QA-AmalNeural', 'gender': 'f'}, {'language': 'ar', 'country': 'qa', 'name': 'ar-QA-MoazNeural', 'gender': 'm'}, {'language': 'ar', 'country': 'sa', 'name': 'ar-SA-HamedNeural', 'gender': 'm'}, {'language': 'ar', 'country': 'sa', 'name': 'ar-SA-ZariyahNeural', 'gender': 'f'}, {'language': 'ar', 'country': 'sy', 'name': 'ar-SY-AmanyNeural', 'gender': 'f'}, {'language': 'ar', 'country': 'sy', 'name': 'ar-SY-LaithNeural', 'gender': 'm'}, {'language': 'ar', 'country': 'tn', 'name': 'ar-TN-HediNeural', 'gender': 'm'}, {'language': 'ar', 'country': 'tn', 'name': 'ar-TN-ReemNeural', 'gender': 'f'}, {'language': 'ar', 'country': 'ye', 'name': 'ar-YE-MaryamNeural', 'gender': 'f'}, {'language': 'ar', 'country': 'ye', 'name': 'ar-YE-SalehNeural', 'gender': 'm'}, {'language': 'az', 'country': 'az', 'name': 'az-AZ-BabekNeural', 'gender': 'm'}, {'language': 'az', 'country': 'az', 'name': 'az-AZ-BanuNeural', 'gender': 'f'}, {'language': 'bg', 'country': 'bg', 'name': 'bg-BG-BorislavNeural', 'gender': 'm'}, {'language': 'bg', 'country': 'bg', 'name': 'bg-BG-KalinaNeural', 'gender': 'f'}, {'language': 'bn', 'country': 'bd', 'name': 'bn-BD-NabanitaNeural', 'gender': 'f'}, {'language': 'bn', 'country': 'bd', 'name': 'bn-BD-PradeepNeural', 'gender': 'm'}, {'language': 'bn', 'country': 'in', 'name': 'bn-IN-BashkarNeural', 'gender': 'm'}, {'language': 'bn', 'country': 'in', 'name': 'bn-IN-TanishaaNeural', 'gender': 'f'}, {'language': 'bs', 'country': 'ba', 'name': 'bs-BA-GoranNeural', 'gender': 'm'}, {'language': 'bs', 'country': 'ba', 'name': 'bs-BA-VesnaNeural', 'gender': 'f'}, {'language': 'ca', 'country': 'es', 'name': 'ca-ES-EnricNeural', 'gender': 'm'}, {'language': 'ca', 'country': 'es', 'name': 'ca-ES-JoanaNeural', 'gender': 'f'}, {'language': 'cs', 'country': 'cz', 'name': 'cs-CZ-AntoninNeural', 'gender': 'm'}, {'language': 'cs', 'country': 'cz', 'name': 'cs-CZ-VlastaNeural', 'gender': 'f'}, {'language': 'cy', 'country': 'gb', 'name': 'cy-GB-AledNeural', 'gender': 'm'}, {'language': 'cy', 'country': 'gb', 'name': 'cy-GB-NiaNeural', 'gender': 'f'}, {'language': 'da', 'country': 'dk', 'name': 'da-DK-ChristelNeural', 'gender': 'f'}, {'language': 'da', 'country': 'dk', 'name': 'da-DK-JeppeNeural', 'gender': 'm'}, {'language': 'de', 'country': 'at', 'name': 'de-AT-IngridNeural', 'gender': 'f'}, {'language': 'de', 'country': 'at', 'name': 'de-AT-JonasNeural', 'gender': 'm'}, {'language': 'de', 'country': 'ch', 'name': 'de-CH-JanNeural', 'gender': 'm'}, {'language': 'de', 'country': 'ch', 'name': 'de-CH-LeniNeural', 'gender': 'f'}, {'language': 'de', 'country': 'de', 'name': 'de-DE-AmalaNeural', 'gender': 'f'}, {'language': 'de', 'country': 'de', 'name': 'de-DE-ConradNeural', 'gender': 'm'}, {'language': 'de', 'country': 'de', 'name': 'de-DE-FlorianMultilingualNeural', 'gender': 'm'}, {'language': 'de', 'country': 'de', 'name': 'de-DE-KatjaNeural', 'gender': 'f'}, {'language': 'de', 'country': 'de', 'name': 'de-DE-KillianNeural', 'gender': 'm'}, {'language': 'de', 'country': 'de', 'name': 'de-DE-SeraphinaMultilingualNeural', 'gender': 'f'}, {'language': 'el', 'country': 'gr', 'name': 'el-GR-AthinaNeural', 'gender': 'f'}, {'language': 'el', 'country': 'gr', 'name': 'el-GR-NestorasNeural', 'gender': 'm'}, {'language': 'en', 'country': 'au', 'name': 'en-AU-NatashaNeural', 'gender': 'f'}, {'language': 'en', 'country': 'au', 'name': 'en-AU-WilliamNeural', 'gender': 'm'}, {'language': 'en', 'country': 'ca', 'name': 'en-CA-ClaraNeural', 'gender': 'f'}, {'language': 'en', 'country': 'ca', 'name': 'en-CA-LiamNeural', 'gender': 'm'}, {'language': 'en', 'country': 'gb', 'name': 'en-GB-LibbyNeural', 'gender': 'f'}, {'language': 'en', 'country': 'gb', 'name': 'en-GB-MaisieNeural', 'gender': 'f'}, {'language': 'en', 'country': 'gb', 'name': 'en-GB-RyanNeural', 'gender': 'm'}, {'language': 'en', 'country': 'gb', 'name': 'en-GB-SoniaNeural', 'gender': 'f'}, {'language': 'en', 'country': 'gb', 'name': 'en-GB-ThomasNeural', 'gender': 'm'}, {'language': 'en', 'country': 'hk', 'name': 'en-HK-SamNeural', 'gender': 'm'}, {'language': 'en', 'country': 'hk', 'name': 'en-HK-YanNeural', 'gender': 'f'}, {'language': 'en', 'country': 'ie', 'name': 'en-IE-ConnorNeural', 'gender': 'm'}, {'language': 'en', 'country': 'ie', 'name': 'en-IE-EmilyNeural', 'gender': 'f'}, {'language': 'en', 'country': 'in', 'name': 'en-IN-NeerjaExpressiveNeural', 'gender': 'f'}, {'language': 'en', 'country': 'in', 'name': 'en-IN-NeerjaNeural', 'gender': 'f'}, {'language': 'en', 'country': 'in', 'name': 'en-IN-PrabhatNeural', 'gender': 'm'}, {'language': 'en', 'country': 'ke', 'name': 'en-KE-AsiliaNeural', 'gender': 'f'}, {'language': 'en', 'country': 'ke', 'name': 'en-KE-ChilembaNeural', 'gender': 'm'}, {'language': 'en', 'country': 'ng', 'name': 'en-NG-AbeoNeural', 'gender': 'm'}, {'language': 'en', 'country': 'ng', 'name': 'en-NG-EzinneNeural', 'gender': 'f'}, {'language': 'en', 'country': 'nz', 'name': 'en-NZ-MitchellNeural', 'gender': 'm'}, {'language': 'en', 'country': 'nz', 'name': 'en-NZ-MollyNeural', 'gender': 'f'}, {'language': 'en', 'country': 'ph', 'name': 'en-PH-JamesNeural', 'gender': 'm'}, {'language': 'en', 'country': 'ph', 'name': 'en-PH-RosaNeural', 'gender': 'f'}, {'language': 'en', 'country': 'sg', 'name': 'en-SG-LunaNeural', 'gender': 'f'}, {'language': 'en', 'country': 'sg', 'name': 'en-SG-WayneNeural', 'gender': 'm'}, {'language': 'en', 'country': 'tz', 'name': 'en-TZ-ElimuNeural', 'gender': 'm'}, {'language': 'en', 'country': 'tz', 'name': 'en-TZ-ImaniNeural', 'gender': 'f'}, {'language': 'en', 'country': 'us', 'name': 'en-US-AnaNeural', 'gender': 'f'}, {'language': 'en', 'country': 'us', 'name': 'en-US-AndrewMultilingualNeural', 'gender': 'm'}, {'language': 'en', 'country': 'us', 'name': 'en-US-AndrewNeural', 'gender': 'm'}, {'language': 'en', 'country': 'us', 'name': 'en-US-AriaNeural', 'gender': 'f'}, {'language': 'en', 'country': 'us', 'name': 'en-US-AvaMultilingualNeural', 'gender': 'f'}, {'language': 'en', 'country': 'us', 'name': 'en-US-AvaNeural', 'gender': 'f'}, {'language': 'en', 'country': 'us', 'name': 'en-US-BrianMultilingualNeural', 'gender': 'm'}, {'language': 'en', 'country': 'us', 'name': 'en-US-BrianNeural', 'gender': 'm'}, {'language': 'en', 'country': 'us', 'name': 'en-US-ChristopherNeural', 'gender': 'm'}, {'language': 'en', 'country': 'us', 'name': 'en-US-EmmaMultilingualNeural', 'gender': 'f'}, {'language': 'en', 'country': 'us', 'name': 'en-US-EmmaNeural', 'gender': 'f'}, {'language': 'en', 'country': 'us', 'name': 'en-US-EricNeural', 'gender': 'm'}, {'language': 'en', 'country': 'us', 'name': 'en-US-GuyNeural', 'gender': 'm'}, {'language': 'en', 'country': 'us', 'name': 'en-US-JennyNeural', 'gender': 'f'}, {'language': 'en', 'country': 'us', 'name': 'en-US-MichelleNeural', 'gender': 'f'}, {'language': 'en', 'country': 'us', 'name': 'en-US-RogerNeural', 'gender': 'm'}, {'language': 'en', 'country': 'us', 'name': 'en-US-SteffanNeural', 'gender': 'm'}, {'language': 'en', 'country': 'za', 'name': 'en-ZA-LeahNeural', 'gender': 'f'}, {'language': 'en', 'country': 'za', 'name': 'en-ZA-LukeNeural', 'gender': 'm'}, {'language': 'es', 'country': 'ar', 'name': 'es-AR-ElenaNeural', 'gender': 'f'}, {'language': 'es', 'country': 'ar', 'name': 'es-AR-TomasNeural', 'gender': 'm'}, {'language': 'es', 'country': 'bo', 'name': 'es-BO-MarceloNeural', 'gender': 'm'}, {'language': 'es', 'country': 'bo', 'name': 'es-BO-SofiaNeural', 'gender': 'f'}, {'language': 'es', 'country': 'cl', 'name': 'es-CL-CatalinaNeural', 'gender': 'f'}, {'language': 'es', 'country': 'cl', 'name': 'es-CL-LorenzoNeural', 'gender': 'm'}, {'language': 'es', 'country': 'co', 'name': 'es-CO-GonzaloNeural', 'gender': 'm'}, {'language': 'es', 'country': 'co', 'name': 'es-CO-SalomeNeural', 'gender': 'f'}, {'language': 'es', 'country': 'cr', 'name': 'es-CR-JuanNeural', 'gender': 'm'}, {'language': 'es', 'country': 'cr', 'name': 'es-CR-MariaNeural', 'gender': 'f'}, {'language': 'es', 'country': 'cu', 'name': 'es-CU-BelkysNeural', 'gender': 'f'}, {'language': 'es', 'country': 'cu', 'name': 'es-CU-ManuelNeural', 'gender': 'm'}, {'language': 'es', 'country': 'do', 'name': 'es-DO-EmilioNeural', 'gender': 'm'}, {'language': 'es', 'country': 'do', 'name': 'es-DO-RamonaNeural', 'gender': 'f'}, {'language': 'es', 'country': 'ec', 'name': 'es-EC-AndreaNeural', 'gender': 'f'}, {'language': 'es', 'country': 'ec', 'name': 'es-EC-LuisNeural', 'gender': 'm'}, {'language': 'es', 'country': 'es', 'name': 'es-ES-AlvaroNeural', 'gender': 'm'}, {'language': 'es', 'country': 'es', 'name': 'es-ES-ElviraNeural', 'gender': 'f'}, {'language': 'es', 'country': 'es', 'name': 'es-ES-XimenaNeural', 'gender': 'f'}, {'language': 'es', 'country': 'gq', 'name': 'es-GQ-JavierNeural', 'gender': 'm'}, {'language': 'es', 'country': 'gq', 'name': 'es-GQ-TeresaNeural', 'gender': 'f'}, {'language': 'es', 'country': 'gt', 'name': 'es-GT-AndresNeural', 'gender': 'm'}, {'language': 'es', 'country': 'gt', 'name': 'es-GT-MartaNeural', 'gender': 'f'}, {'language': 'es', 'country': 'hn', 'name': 'es-HN-CarlosNeural', 'gender': 'm'}, {'language': 'es', 'country': 'hn', 'name': 'es-HN-KarlaNeural', 'gender': 'f'}, {'language': 'es', 'country': 'mx', 'name': 'es-MX-DaliaNeural', 'gender': 'f'}, {'language': 'es', 'country': 'mx', 'name': 'es-MX-JorgeNeural', 'gender': 'm'}, {'language': 'es', 'country': 'ni', 'name': 'es-NI-FedericoNeural', 'gender': 'm'}, {'language': 'es', 'country': 'ni', 'name': 'es-NI-YolandaNeural', 'gender': 'f'}, {'language': 'es', 'country': 'pa', 'name': 'es-PA-MargaritaNeural', 'gender': 'f'}, {'language': 'es', 'country': 'pa', 'name': 'es-PA-RobertoNeural', 'gender': 'm'}, {'language': 'es', 'country': 'pe', 'name': 'es-PE-AlexNeural', 'gender': 'm'}, {'language': 'es', 'country': 'pe', 'name': 'es-PE-CamilaNeural', 'gender': 'f'}, {'language': 'es', 'country': 'pr', 'name': 'es-PR-KarinaNeural', 'gender': 'f'}, {'language': 'es', 'country': 'pr', 'name': 'es-PR-VictorNeural', 'gender': 'm'}, {'language': 'es', 'country': 'py', 'name': 'es-PY-MarioNeural', 'gender': 'm'}, {'language': 'es', 'country': 'py', 'name': 'es-PY-TaniaNeural', 'gender': 'f'}, {'language': 'es', 'country': 'sv', 'name': 'es-SV-LorenaNeural', 'gender': 'f'}, {'language': 'es', 'country': 'sv', 'name': 'es-SV-RodrigoNeural', 'gender': 'm'}, {'language': 'es', 'country': 'us', 'name': 'es-US-AlonsoNeural', 'gender': 'm'}, {'language': 'es', 'country': 'us', 'name': 'es-US-PalomaNeural', 'gender': 'f'}, {'language': 'es', 'country': 'uy', 'name': 'es-UY-MateoNeural', 'gender': 'm'}, {'language': 'es', 'country': 'uy', 'name': 'es-UY-ValentinaNeural', 'gender': 'f'}, {'language': 'es', 'country': 've', 'name': 'es-VE-PaolaNeural', 'gender': 'f'}, {'language': 'es', 'country': 've', 'name': 'es-VE-SebastianNeural', 'gender': 'm'}, {'language': 'et', 'country': 'ee', 'name': 'et-EE-AnuNeural', 'gender': 'f'}, {'language': 'et', 'country': 'ee', 'name': 'et-EE-KertNeural', 'gender': 'm'}, {'language': 'fa', 'country': 'ir', 'name': 'fa-IR-DilaraNeural', 'gender': 'f'}, {'language': 'fa', 'country': 'ir', 'name': 'fa-IR-FaridNeural', 'gender': 'm'}, {'language': 'fi', 'country': 'fi', 'name': 'fi-FI-HarriNeural', 'gender': 'm'}, {'language': 'fi', 'country': 'fi', 'name': 'fi-FI-NooraNeural', 'gender': 'f'}, {'language': 'fil', 'country': 'ph', 'name': 'fil-PH-AngeloNeural', 'gender': 'm'}, {'language': 'fil', 'country': 'ph', 'name': 'fil-PH-BlessicaNeural', 'gender': 'f'}, {'language': 'fr', 'country': 'be', 'name': 'fr-BE-CharlineNeural', 'gender': 'f'}, {'language': 'fr', 'country': 'be', 'name': 'fr-BE-GerardNeural', 'gender': 'm'}, {'language': 'fr', 'country': 'ca', 'name': 'fr-CA-AntoineNeural', 'gender': 'm'}, {'language': 'fr', 'country': 'ca', 'name': 'fr-CA-JeanNeural', 'gender': 'm'}, {'language': 'fr', 'country': 'ca', 'name': 'fr-CA-SylvieNeural', 'gender': 'f'}, {'language': 'fr', 'country': 'ca', 'name': 'fr-CA-ThierryNeural', 'gender': 'm'}, {'language': 'fr', 'country': 'ch', 'name': 'fr-CH-ArianeNeural', 'gender': 'f'}, {'language': 'fr', 'country': 'ch', 'name': 'fr-CH-FabriceNeural', 'gender': 'm'}, {'language': 'fr', 'country': 'fr', 'name': 'fr-FR-DeniseNeural', 'gender': 'f'}, {'language': 'fr', 'country': 'fr', 'name': 'fr-FR-EloiseNeural', 'gender': 'f'}, {'language': 'fr', 'country': 'fr', 'name': 'fr-FR-HenriNeural', 'gender': 'm'}, {'language': 'fr', 'country': 'fr', 'name': 'fr-FR-RemyMultilingualNeural', 'gender': 'm'}, {'language': 'fr', 'country': 'fr', 'name': 'fr-FR-VivienneMultilingualNeural', 'gender': 'f'}, {'language': 'ga', 'country': 'ie', 'name': 'ga-IE-ColmNeural', 'gender': 'm'}, {'language': 'ga', 'country': 'ie', 'name': 'ga-IE-OrlaNeural', 'gender': 'f'}, {'language': 'gl', 'country': 'es', 'name': 'gl-ES-RoiNeural', 'gender': 'm'}, {'language': 'gl', 'country': 'es', 'name': 'gl-ES-SabelaNeural', 'gender': 'f'}, {'language': 'gu', 'country': 'in', 'name': 'gu-IN-DhwaniNeural', 'gender': 'f'}, {'language': 'gu', 'country': 'in', 'name': 'gu-IN-NiranjanNeural', 'gender': 'm'}, {'language': 'he', 'country': 'il', 'name': 'he-IL-AvriNeural', 'gender': 'm'}, {'language': 'he', 'country': 'il', 'name': 'he-IL-HilaNeural', 'gender': 'f'}, {'language': 'hi', 'country': 'in', 'name': 'hi-IN-MadhurNeural', 'gender': 'm'}, {'language': 'hi', 'country': 'in', 'name': 'hi-IN-SwaraNeural', 'gender': 'f'}, {'language': 'hr', 'country': 'hr', 'name': 'hr-HR-GabrijelaNeural', 'gender': 'f'}, {'language': 'hr', 'country': 'hr', 'name': 'hr-HR-SreckoNeural', 'gender': 'm'}, {'language': 'hu', 'country': 'hu', 'name': 'hu-HU-NoemiNeural', 'gender': 'f'}, {'language': 'hu', 'country': 'hu', 'name': 'hu-HU-TamasNeural', 'gender': 'm'}, {'language': 'id', 'country': 'id', 'name': 'id-ID-ArdiNeural', 'gender': 'm'}, {'language': 'id', 'country': 'id', 'name': 'id-ID-GadisNeural', 'gender': 'f'}, {'language': 'is', 'country': 'is', 'name': 'is-IS-GudrunNeural', 'gender': 'f'}, {'language': 'is', 'country': 'is', 'name': 'is-IS-GunnarNeural', 'gender': 'm'}, {'language': 'it', 'country': 'it', 'name': 'it-IT-DiegoNeural', 'gender': 'm'}, {'language': 'it', 'country': 'it', 'name': 'it-IT-ElsaNeural', 'gender': 'f'}, {'language': 'it', 'country': 'it', 'name': 'it-IT-GiuseppeNeural', 'gender': 'm'}, {'language': 'it', 'country': 'it', 'name': 'it-IT-IsabellaNeural', 'gender': 'f'}, {'language': 'ja', 'country': 'jp', 'name': 'ja-JP-KeitaNeural', 'gender': 'm'}, {'language': 'ja', 'country': 'jp', 'name': 'ja-JP-NanamiNeural', 'gender': 'f'}, {'language': 'jv', 'country': 'id', 'name': 'jv-ID-DimasNeural', 'gender': 'm'}, {'language': 'jv', 'country': 'id', 'name': 'jv-ID-SitiNeural', 'gender': 'f'}, {'language': 'ka', 'country': 'ge', 'name': 'ka-GE-EkaNeural', 'gender': 'f'}, {'language': 'ka', 'country': 'ge', 'name': 'ka-GE-GiorgiNeural', 'gender': 'm'}, {'language': 'kk', 'country': 'kz', 'name': 'kk-KZ-AigulNeural', 'gender': 'f'}, {'language': 'kk', 'country': 'kz', 'name': 'kk-KZ-DauletNeural', 'gender': 'm'}, {'language': 'km', 'country': 'kh', 'name': 'km-KH-PisethNeural', 'gender': 'm'}, {'language': 'km', 'country': 'kh', 'name': 'km-KH-SreymomNeural', 'gender': 'f'}, {'language': 'kn', 'country': 'in', 'name': 'kn-IN-GaganNeural', 'gender': 'm'}, {'language': 'kn', 'country': 'in', 'name': 'kn-IN-SapnaNeural', 'gender': 'f'}, {'language': 'ko', 'country': 'kr', 'name': 'ko-KR-HyunsuNeural', 'gender': 'm'}, {'language': 'ko', 'country': 'kr', 'name': 'ko-KR-InJoonNeural', 'gender': 'm'}, {'language': 'ko', 'country': 'kr', 'name': 'ko-KR-SunHiNeural', 'gender': 'f'}, {'language': 'lo', 'country': 'la', 'name': 'lo-LA-ChanthavongNeural', 'gender': 'm'}, {'language': 'lo', 'country': 'la', 'name': 'lo-LA-KeomanyNeural', 'gender': 'f'}, {'language': 'lt', 'country': 'lt', 'name': 'lt-LT-LeonasNeural', 'gender': 'm'}, {'language': 'lt', 'country': 'lt', 'name': 'lt-LT-OnaNeural', 'gender': 'f'}, {'language': 'lv', 'country': 'lv', 'name': 'lv-LV-EveritaNeural', 'gender': 'f'}, {'language': 'lv', 'country': 'lv', 'name': 'lv-LV-NilsNeural', 'gender': 'm'}, {'language': 'mk', 'country': 'mk', 'name': 'mk-MK-AleksandarNeural', 'gender': 'm'}, {'language': 'mk', 'country': 'mk', 'name': 'mk-MK-MarijaNeural', 'gender': 'f'}, {'language': 'ml', 'country': 'in', 'name': 'ml-IN-MidhunNeural', 'gender': 'm'}, {'language': 'ml', 'country': 'in', 'name': 'ml-IN-SobhanaNeural', 'gender': 'f'}, {'language': 'mn', 'country': 'mn', 'name': 'mn-MN-BataaNeural', 'gender': 'm'}, {'language': 'mn', 'country': 'mn', 'name': 'mn-MN-YesuiNeural', 'gender': 'f'}, {'language': 'mr', 'country': 'in', 'name': 'mr-IN-AarohiNeural', 'gender': 'f'}, {'language': 'mr', 'country': 'in', 'name': 'mr-IN-ManoharNeural', 'gender': 'm'}, {'language': 'ms', 'country': 'my', 'name': 'ms-MY-OsmanNeural', 'gender': 'm'}, {'language': 'ms', 'country': 'my', 'name': 'ms-MY-YasminNeural', 'gender': 'f'}, {'language': 'mt', 'country': 'mt', 'name': 'mt-MT-GraceNeural', 'gender': 'f'}, {'language': 'mt', 'country': 'mt', 'name': 'mt-MT-JosephNeural', 'gender': 'm'}, {'language': 'my', 'country': 'mm', 'name': 'my-MM-NilarNeural', 'gender': 'f'}, {'language': 'my', 'country': 'mm', 'name': 'my-MM-ThihaNeural', 'gender': 'm'}, {'language': 'nb', 'country': 'no', 'name': 'nb-NO-FinnNeural', 'gender': 'm'}, {'language': 'nb', 'country': 'no', 'name': 'nb-NO-PernilleNeural', 'gender': 'f'}, {'language': 'ne', 'country': 'np', 'name': 'ne-NP-HemkalaNeural', 'gender': 'f'}, {'language': 'ne', 'country': 'np', 'name': 'ne-NP-SagarNeural', 'gender': 'm'}, {'language': 'nl', 'country': 'be', 'name': 'nl-BE-ArnaudNeural', 'gender': 'm'}, {'language': 'nl', 'country': 'be', 'name': 'nl-BE-DenaNeural', 'gender': 'f'}, {'language': 'nl', 'country': 'nl', 'name': 'nl-NL-ColetteNeural', 'gender': 'f'}, {'language': 'nl', 'country': 'nl', 'name': 'nl-NL-FennaNeural', 'gender': 'f'}, {'language': 'nl', 'country': 'nl', 'name': 'nl-NL-MaartenNeural', 'gender': 'm'}, {'language': 'pl', 'country': 'pl', 'name': 'pl-PL-MarekNeural', 'gender': 'm'}, {'language': 'pl', 'country': 'pl', 'name': 'pl-PL-ZofiaNeural', 'gender': 'f'}, {'language': 'ps', 'country': 'af', 'name': 'ps-AF-GulNawazNeural', 'gender': 'm'}, {'language': 'ps', 'country': 'af', 'name': 'ps-AF-LatifaNeural', 'gender': 'f'}, {'language': 'pt', 'country': 'br', 'name': 'pt-BR-AntonioNeural', 'gender': 'm'}, {'language': 'pt', 'country': 'br', 'name': 'pt-BR-FranciscaNeural', 'gender': 'f'}, {'language': 'pt', 'country': 'br', 'name': 'pt-BR-ThalitaNeural', 'gender': 'f'}, {'language': 'pt', 'country': 'pt', 'name': 'pt-PT-DuarteNeural', 'gender': 'm'}, {'language': 'pt', 'country': 'pt', 'name': 'pt-PT-RaquelNeural', 'gender': 'f'}, {'language': 'ro', 'country': 'ro', 'name': 'ro-RO-AlinaNeural', 'gender': 'f'}, {'language': 'ro', 'country': 'ro', 'name': 'ro-RO-EmilNeural', 'gender': 'm'}, {'language': 'ru', 'country': 'ru', 'name': 'ru-RU-DmitryNeural', 'gender': 'm'}, {'language': 'ru', 'country': 'ru', 'name': 'ru-RU-SvetlanaNeural', 'gender': 'f'}, {'language': 'si', 'country': 'lk', 'name': 'si-LK-SameeraNeural', 'gender': 'm'}, {'language': 'si', 'country': 'lk', 'name': 'si-LK-ThiliniNeural', 'gender': 'f'}, {'language': 'sk', 'country': 'sk', 'name': 'sk-SK-LukasNeural', 'gender': 'm'}, {'language': 'sk', 'country': 'sk', 'name': 'sk-SK-ViktoriaNeural', 'gender': 'f'}, {'language': 'sl', 'country': 'si', 'name': 'sl-SI-PetraNeural', 'gender': 'f'}, {'language': 'sl', 'country': 'si', 'name': 'sl-SI-RokNeural', 'gender': 'm'}, {'language': 'so', 'country': 'so', 'name': 'so-SO-MuuseNeural', 'gender': 'm'}, {'language': 'so', 'country': 'so', 'name': 'so-SO-UbaxNeural', 'gender': 'f'}, {'language': 'sq', 'country': 'al', 'name': 'sq-AL-AnilaNeural', 'gender': 'f'}, {'language': 'sq', 'country': 'al', 'name': 'sq-AL-IlirNeural', 'gender': 'm'}, {'language': 'sr', 'country': 'rs', 'name': 'sr-RS-NicholasNeural', 'gender': 'm'}, {'language': 'sr', 'country': 'rs', 'name': 'sr-RS-SophieNeural', 'gender': 'f'}, {'language': 'su', 'country': 'id', 'name': 'su-ID-JajangNeural', 'gender': 'm'}, {'language': 'su', 'country': 'id', 'name': 'su-ID-TutiNeural', 'gender': 'f'}, {'language': 'sv', 'country': 'se', 'name': 'sv-SE-MattiasNeural', 'gender': 'm'}, {'language': 'sv', 'country': 'se', 'name': 'sv-SE-SofieNeural', 'gender': 'f'}, {'language': 'sw', 'country': 'ke', 'name': 'sw-KE-RafikiNeural', 'gender': 'm'}, {'language': 'sw', 'country': 'ke', 'name': 'sw-KE-ZuriNeural', 'gender': 'f'}, {'language': 'sw', 'country': 'tz', 'name': 'sw-TZ-DaudiNeural', 'gender': 'm'}, {'language': 'sw', 'country': 'tz', 'name': 'sw-TZ-RehemaNeural', 'gender': 'f'}, {'language': 'ta', 'country': 'in', 'name': 'ta-IN-PallaviNeural', 'gender': 'f'}, {'language': 'ta', 'country': 'in', 'name': 'ta-IN-ValluvarNeural', 'gender': 'm'}, {'language': 'ta', 'country': 'lk', 'name': 'ta-LK-KumarNeural', 'gender': 'm'}, {'language': 'ta', 'country': 'lk', 'name': 'ta-LK-SaranyaNeural', 'gender': 'f'}, {'language': 'ta', 'country': 'my', 'name': 'ta-MY-KaniNeural', 'gender': 'f'}, {'language': 'ta', 'country': 'my', 'name': 'ta-MY-SuryaNeural', 'gender': 'm'}, {'language': 'ta', 'country': 'sg', 'name': 'ta-SG-AnbuNeural', 'gender': 'm'}, {'language': 'ta', 'country': 'sg', 'name': 'ta-SG-VenbaNeural', 'gender': 'f'}, {'language': 'te', 'country': 'in', 'name': 'te-IN-MohanNeural', 'gender': 'm'}, {'language': 'te', 'country': 'in', 'name': 'te-IN-ShrutiNeural', 'gender': 'f'}, {'language': 'th', 'country': 'th', 'name': 'th-TH-NiwatNeural', 'gender': 'm'}, {'language': 'th', 'country': 'th', 'name': 'th-TH-PremwadeeNeural', 'gender': 'f'}, {'language': 'tr', 'country': 'tr', 'name': 'tr-TR-AhmetNeural', 'gender': 'm'}, {'language': 'tr', 'country': 'tr', 'name': 'tr-TR-EmelNeural', 'gender': 'f'}, {'language': 'uk', 'country': 'ua', 'name': 'uk-UA-OstapNeural', 'gender': 'm'}, {'language': 'uk', 'country': 'ua', 'name': 'uk-UA-PolinaNeural', 'gender': 'f'}, {'language': 'ur', 'country': 'in', 'name': 'ur-IN-GulNeural', 'gender': 'f'}, {'language': 'ur', 'country': 'in', 'name': 'ur-IN-SalmanNeural', 'gender': 'm'}, {'language': 'ur', 'country': 'pk', 'name': 'ur-PK-AsadNeural', 'gender': 'm'}, {'language': 'ur', 'country': 'pk', 'name': 'ur-PK-UzmaNeural', 'gender': 'f'}, {'language': 'uz', 'country': 'uz', 'name': 'uz-UZ-MadinaNeural', 'gender': 'f'}, {'language': 'uz', 'country': 'uz', 'name': 'uz-UZ-SardorNeural', 'gender': 'm'}, {'language': 'vi', 'country': 'vn', 'name': 'vi-VN-HoaiMyNeural', 'gender': 'f'}, {'language': 'vi', 'country': 'vn', 'name': 'vi-VN-NamMinhNeural', 'gender': 'm'}, {'language': 'zh', 'country': 'cn', 'name': 'zh-CN-XiaoxiaoNeural', 'gender': 'f'}, {'language': 'zh', 'country': 'cn', 'name': 'zh-CN-XiaoyiNeural', 'gender': 'f'}, {'language': 'zh', 'country': 'cn', 'name': 'zh-CN-YunjianNeural', 'gender': 'm'}, {'language': 'zh', 'country': 'cn', 'name': 'zh-CN-YunxiNeural', 'gender': 'm'}, {'language': 'zh', 'country': 'cn', 'name': 'zh-CN-YunxiaNeural', 'gender': 'm'}, {'language': 'zh', 'country': 'cn', 'name': 'zh-CN-YunyangNeural', 'gender': 'm'}, {'language': 'zh', 'country': 'cn', 'name': 'zh-CN-liaoning-XiaobeiNeural', 'gender': 'f'}, {'language': 'zh', 'country': 'cn', 'name': 'zh-CN-shaanxi-XiaoniNeural', 'gender': 'f'}, {'language': 'zh', 'country': 'hk', 'name': 'zh-HK-HiuGaaiNeural', 'gender': 'f'}, {'language': 'zh', 'country': 'hk', 'name': 'zh-HK-HiuMaanNeural', 'gender': 'f'}, {'language': 'zh', 'country': 'hk', 'name': 'zh-HK-WanLungNeural', 'gender': 'm'}, {'language': 'zh', 'country': 'tw', 'name': 'zh-TW-HsiaoChenNeural', 'gender': 'f'}, {'language': 'zh', 'country': 'tw', 'name': 'zh-TW-HsiaoYuNeural', 'gender': 'f'}, {'language': 'zh', 'country': 'tw', 'name': 'zh-TW-YunJheNeural', 'gender': 'm'}, {'language': 'zu', 'country': 'za', 'name': 'zu-ZA-ThandoNeural', 'gender': 'f'}, {'language': 'zu', 'country': 'za', 'name': 'zu-ZA-ThembaNeural', 'gender': 'm'}]\n"
          ]
        }
      ],
      "source": [
        "command = \"edge-tts --list-voices\"\n",
        "\n",
        "# Execute the command and store the output in a list\n",
        "voice_list = execute_command(command)\n",
        "\n",
        "# Remove empty strings using list comprehension\n",
        "filtered_list = [x for x in voice_list if x != \"\"]\n",
        "\n",
        "# Initialize an empty list to store dictionaries\n",
        "result = []\n",
        "\n",
        "# Iterate over the list of strings\n",
        "for i in range(0, len(filtered_list), 2):\n",
        "    # Extract name and gender\n",
        "    name = filtered_list[i].split(': ')[1]\n",
        "    gender = filtered_list[i + 1].split(': ')[1][0].lower()  # Extract first character and convert to lowercase\n",
        "\n",
        "    # Create dictionary and append to result list\n",
        "    result.append({'name': name, 'gender': gender})\n",
        "\n",
        "voice_list = []\n",
        "\n",
        "for item in result:\n",
        "    name_parts = item['name'].split('-')\n",
        "    language = name_parts[0]\n",
        "    country = name_parts[1]\n",
        "    item_with_language_country = {\n",
        "        'language': language,\n",
        "        'country': country.lower(),\n",
        "        'name': item['name'],\n",
        "        'gender': item['gender']\n",
        "    }\n",
        "    voice_list.append(item_with_language_country)\n",
        "\n",
        "print(voice_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHgwYEfMdOhx"
      },
      "outputs": [],
      "source": [
        "def speak_preparation(output_list):\n",
        "  '''Fill in the empty speakers in the transcript'''\n",
        "\n",
        "  if output_list[0]['speaker'] == '':\n",
        "    output_list[0]['speaker'] = 'm1'\n",
        "\n",
        "  for i in range(len(output_list)):\n",
        "    if output_list[i]['speaker'] == '':\n",
        "      output_list[i]['speaker'] = output_list[i-1]['speaker']\n",
        "\n",
        "  return output_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ftK6B0zj2_g"
      },
      "outputs": [],
      "source": [
        "def speak_preparation2(languageCode, transcript):\n",
        "  '''Make sure the number of speakers is not more than the number of available voices'''\n",
        "\n",
        "  if languageCode == 'zh-Hans':\n",
        "    languageCode = 'zh'\n",
        "\n",
        "  # Extract unique 'speaker' values\n",
        "  unique_speakers = set()\n",
        "  for line in transcript:\n",
        "    unique_speakers.add(line['speaker'])\n",
        "\n",
        "  unique_speakers = list(unique_speakers)\n",
        "\n",
        "  if len(unique_speakers) > 1:\n",
        "    maleSpeakers = []\n",
        "    femaleSpeakers = []\n",
        "    for item in unique_speakers:\n",
        "      if item.startswith('m'):\n",
        "        maleSpeakers.append(item)\n",
        "      elif item.startswith('f'):\n",
        "        femaleSpeakers.append(item)\n",
        "\n",
        "    # Count the number of voices with 'language' key equal to languageCode\n",
        "    maleVoiceCount = sum(1 for voice in voice_list if voice.get('language') == languageCode and voice.get('gender') == 'm')\n",
        "    femaleVoiceCount = sum(1 for voice in voice_list if voice.get('language') == languageCode and voice.get('gender') == 'f')\n",
        "\n",
        "    if (maleVoiceCount == 0 and len(maleSpeakers) > 0) or (femaleVoiceCount == 0 and len(femaleSpeakers) > 0):\n",
        "      return 'Voice not found'\n",
        "\n",
        "    while maleVoiceCount < len(maleSpeakers):\n",
        "      maleSpeakers.pop(-1)\n",
        "\n",
        "    while femaleVoiceCount < len(femaleSpeakers):\n",
        "      femaleSpeakers.pop(-1)\n",
        "\n",
        "    for line in transcript:\n",
        "      if line['speaker'][0] == 'm' and (line['speaker'] not in maleSpeakers):\n",
        "        line['speaker'] = 'm1'\n",
        "      if line['speaker'][0] == 'f' and (line['speaker'] not in femaleSpeakers):\n",
        "        line['speaker'] = 'f1'\n",
        "\n",
        "  return transcript"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vb8m7Bvz01Yz"
      },
      "outputs": [],
      "source": [
        "def speak_preparation3(languageCode, transcript):\n",
        "  '''Assign the voices to the lines in transcript'''\n",
        "\n",
        "  if languageCode == 'zh-Hans':\n",
        "    languageCode = 'zh'\n",
        "\n",
        "  # selectedVoiceList = []\n",
        "\n",
        "  speakers = {'m1': [], 'f1': [], 'm2': [], 'f2': [], 'm3': [], 'f3': [], 'm4': [], 'f4': [], 'm5': [], 'f5': [], 'm6': [], 'f6': []}\n",
        "\n",
        "  for line in transcript:\n",
        "    key = line.get('speaker')\n",
        "    if key in speakers:\n",
        "      speakers[key].append(line)\n",
        "\n",
        "  speakerKeys = ['m1', 'f1', 'm2', 'f2', 'm3', 'f3', 'm4', 'f4', 'm5', 'f5', 'm6', 'f6']\n",
        "  for key in speakerKeys:\n",
        "    speakerNumber = int(key[1])\n",
        "    if len(speakers[key]) > 0:\n",
        "      for line in speakers[key]:\n",
        "        for voice in voice_list:\n",
        "          if voice['language'] == languageCode and voice['gender'] == line['speaker'][0]:\n",
        "            if speakerNumber == 1:\n",
        "              line['voice'] = voice['name']\n",
        "              break\n",
        "            else:\n",
        "              speakerNumber -= 1\n",
        "\n",
        "  # Flatten the dictionaries into a single list\n",
        "  all_dicts = [item for sublist in speakers.values() for item in sublist]\n",
        "\n",
        "  # Sort the list based on the value of 'start' key in each dictionary\n",
        "  transcriptWithVoice = sorted(all_dicts, key=lambda x: x['start'])\n",
        "\n",
        "  return transcriptWithVoice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aR1hxwnKPG9I"
      },
      "outputs": [],
      "source": [
        "def speak_preparation4(transcriptWithVoice):\n",
        "  '''Text-to-speech starts'''\n",
        "\n",
        "  i = 0\n",
        "  speeches = []\n",
        "  for line in transcriptWithVoice:\n",
        "    # Convert text to speech with estimated duration\n",
        "    if line['text'] != '':\n",
        "      text = line['text']\n",
        "      voice = line['voice']\n",
        "      rate = 0\n",
        "      command = f'edge-tts --voice {voice} --text \"{text}\" --write-media {\"/content/audio/tts\" + str(i) + \".mp3\"}'\n",
        "      subprocess.run(command, shell=True, check=True)\n",
        "      audio_file = f\"/content/audio/tts{str(i)}.mp3\"\n",
        "      audio = AudioSegment.from_file(audio_file)\n",
        "      duration = len(audio)/1000\n",
        "      rate = 10\n",
        "      while duration > line['duration']:\n",
        "        os.remove(audio_file)\n",
        "        command = f'edge-tts --rate=+{rate}% --voice {voice} --text \"{text}\" --write-media {\"/content/audio/tts\" + str(i) + \".mp3\"}'\n",
        "        subprocess.run(command, shell=True, check=True)\n",
        "        audio_file = f\"/content/audio/tts{str(i)}.mp3\"\n",
        "        audio = AudioSegment.from_file(audio_file)\n",
        "        duration = len(audio)/1000\n",
        "        rate += 10\n",
        "        if rate > 200:\n",
        "          break\n",
        "    else:\n",
        "      audio = AudioSegment.silent(duration=line['duration']*1000)\n",
        "    speeches.append(audio)\n",
        "    i += 1\n",
        "  # return len(speech)\n",
        "\n",
        "  if transcriptWithVoice[0]['start'] == 0:\n",
        "    speech = speeches[0]\n",
        "  else:\n",
        "    speech = AudioSegment.silent(duration=transcriptWithVoice[0]['start']*1000)\n",
        "    speech = speech + speeches[0]\n",
        "\n",
        "  length = len(speech)\n",
        "  silenceDuration = transcriptWithVoice[1]['start']*1000 - length\n",
        "  if silenceDuration > 0:\n",
        "    speech = speech + AudioSegment.silent(duration=silenceDuration)\n",
        "\n",
        "  for j in range(1, len(speeches)-1):\n",
        "    speech = speech + speeches[j]\n",
        "    length = len(speech)\n",
        "    silenceDuration = transcriptWithVoice[j+1]['start']*1000 - length\n",
        "    if silenceDuration > 0:\n",
        "      speech = speech + AudioSegment.silent(duration=silenceDuration)\n",
        "\n",
        "  speech = speech + speeches[-1]\n",
        "  speech.export(\"/content/speech.mp3\", format=\"mp3\")\n",
        "  return speech"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCqQckwZ4spn"
      },
      "outputs": [],
      "source": [
        "@anvil.server.callable\n",
        "def speak(languageCode, user, videoID):\n",
        "  if os.path.exists('/content/transcript.srt'):\n",
        "    os.remove('/content/transcript.srt')\n",
        "\n",
        "  videoRow = app_tables.videos.get(youTubeVideoID=videoID)\n",
        "  transcriptRow = app_tables.transcripts.get(createdBy=user, languageCode=languageCode, video=videoRow)\n",
        "  transcript = transcriptRow['transcript']\n",
        "  output_list = []\n",
        "\n",
        "  for item in transcript:\n",
        "    item['startMin'] = int(item['startMin'])\n",
        "    item['startSec'] = int(item['startSec'])\n",
        "    item['endMin'] = int(item['endMin'])\n",
        "    item['endSec'] = int(item['endSec'])\n",
        "    duration = (item['endMin'] * 60 + item['endSec']) - (item['startMin'] * 60 + item['startSec'])\n",
        "    new_item = {\n",
        "        \"text\": item[\"text\"],\n",
        "        \"speaker\": item[\"speaker\"],\n",
        "        \"start\": item['startMin'] * 60 + item['startSec'],\n",
        "        \"end\": item['endMin'] * 60 + item['endSec'],\n",
        "        \"duration\": duration\n",
        "    }\n",
        "    output_list.append(new_item)\n",
        "\n",
        "  if output_list[0]['start'] < 0:\n",
        "    output_list[0]['start'] = 0\n",
        "\n",
        "  transcript = speak_preparation(output_list)\n",
        "  transcript = speak_preparation2(languageCode, transcript)\n",
        "  if transcript == 'Voice not found':\n",
        "    return 'Voice not found'\n",
        "\n",
        "  directory_name = \"audio\"\n",
        "  if not os.path.exists(directory_name):\n",
        "    os.system(f\"mkdir {directory_name}\")\n",
        "  else:\n",
        "    os.system(f\"rm -r {directory_name}\")\n",
        "    os.system(f\"mkdir {directory_name}\")\n",
        "\n",
        "  if os.path.exists('/content/speech.mp3'):\n",
        "    os.remove('/content/speech.mp3')\n",
        "\n",
        "  transcriptWithVoice = speak_preparation3(languageCode, transcript)\n",
        "\n",
        "  speech = speak_preparation4(transcriptWithVoice)\n",
        "\n",
        "  accompaniment = AudioSegment.from_file(io.BytesIO(videoRow['accompaniment'].get_bytes()), format=\"mp3\")\n",
        "\n",
        "  # Ensure both audio tracks start at the same time\n",
        "  speech = speech.set_frame_rate(accompaniment.frame_rate)\n",
        "  speech = speech.set_channels(accompaniment.channels)\n",
        "  speech = speech.set_sample_width(accompaniment.sample_width)\n",
        "\n",
        "  # Overlay the audio onto the background\n",
        "  dub = accompaniment.overlay(speech)\n",
        "  # Export the resulting audio\n",
        "  dub.export(\"dub.mp3\", format=\"mp3\")\n",
        "  with open(\"/content/dub.mp3\", \"rb\") as f:\n",
        "    audioBytes = f.read()\n",
        "  audio = anvil.BlobMedia(content_type=\"audio/mp3\", content=audioBytes, name='dub.mp3')\n",
        "  transcriptRow.update(dub=audio, accent='Machine')\n",
        "  os.system(\"rm -r audio\")\n",
        "  os.remove('/content/speech.mp3')\n",
        "  os.remove('/content/dub.mp3')\n",
        "\n",
        "  return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doIYP1vDoTRN"
      },
      "outputs": [],
      "source": [
        "# user = app_tables.users.get(userID='u2')\n",
        "# speak('en', user, '2bdvoVr64HI')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmKhTMstLaf6"
      },
      "source": [
        "#Sandbox"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cveV5jvAUdSq"
      },
      "source": [
        "## Speaker Diarization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06WFrup7PhVB"
      },
      "source": [
        "## Clone the voice"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "pz6_A7JnSeI9",
        "RTcxww1icURg",
        "J2UfeyTicDV2",
        "JTtBWHp4ko9P",
        "5F2PuK6NNzau",
        "9MPiExhkVzVj",
        "fJnHIPD5fTfY"
      ],
      "provenance": [],
      "toc_visible": true,
      "gpuType": "L4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}